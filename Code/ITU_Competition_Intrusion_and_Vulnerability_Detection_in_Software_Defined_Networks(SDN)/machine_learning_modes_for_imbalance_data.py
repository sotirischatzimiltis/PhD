################### IMPORT NECESSARY PACKAGES ############################################
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
import seaborn as sns
import matplotlib.pyplot as plt
import imbens
from imbens.metrics import geometric_mean_score
from imbens.metrics import classification_report_imbalanced
from sklearn import preprocessing
from sklearn import tree
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
################### METHOD THAT PRINTS ALL EVALUATION METRICS NEEDED #####################
def perf_evaluation(y_true,y_pred,class_labels):
    # # Compute the confusion matrix
    matrix = confusion_matrix(y_true,y_pred,normalize='true')
    conf_matrix = confusion_matrix(y_true,y_pred)
    print("####################################################################################")
    print("Confusion Matrix:")
    # Plot the confusion matrix as a heatmap
    plt.figure(figsize=(15, 15))
    sns.heatmap(matrix, annot=True,fmt='.4f',cmap="viridis", square=True,
                xticklabels=class_labels, yticklabels=class_labels)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix')
    plt.show()
    # Accuracy and Classification Report
    accuracy = accuracy_score(y_true, y_pred) * 100
    print("Total Accuracy: ",accuracy)
    report = classification_report(y_true,y_pred)
    print("Classification Report")
    print(report)
    # Classification Report based on the imbens library
    report = classification_report_imbalanced(y_true, y_pred)
    print("Classification Report based on imbens")
    print(report)
    # Calculate TP, TN, FP, FN for each class
    class_tp = {}
    class_tn = {}
    class_fp = {}
    class_fn = {}
    for i, label in enumerate(class_labels):
        tp = conf_matrix[i, i]
        tn = conf_matrix.sum() - conf_matrix[i, :].sum() - conf_matrix[:, i].sum() + tp
        fp = conf_matrix[:, i].sum() - tp
        fn = conf_matrix[i, :].sum() - tp
        class_tp[label] = tp
        class_tn[label] = tn
        class_fp[label] = fp
        class_fn[label] = fn
    # Print true positives, true negatives, false positives, and false negatives for each class
    for label in class_labels:
        print(f"Class {label}:")
        print("True Positives (TP):", class_tp[label])
        print("True Negatives (TN):", class_tn[label])
        print("False Positives (FP):", class_fp[label])
        print("False Negatives (FN):", class_fn[label])
        print()


# ########################## MAIN CODE ####################################################
# ################### READ DATASETS #######################################################
train_file_path = 'train_data_no_duplicates.csv'
test_file_path = 'test_data_no_duplicates.csv'
train_df = pd.read_csv(train_file_path,skipinitialspace=True)
test_df = pd.read_csv(test_file_path,skipinitialspace=True)
# Split data into X(input features) and Y (labels)
X_train = train_df.drop(train_df.columns[-1], axis=1)
y_train = train_df[train_df.columns[-1]]
X_test = test_df.drop(test_df.columns[-1], axis=1)
y_test = test_df[test_df.columns[-1]]
# Scale data
scaler = preprocessing.StandardScaler()
scaler.fit(X_train)
x_train = scaler.transform(X_train)
x_test = scaler.transform(X_test)
# Save feature and class names
features = X_train.columns.tolist()
class_labels = ['Benign','Bot','DDoS','DoS_Golden_Eye','DoS_Hulk','DoS_Slowhttptest','DoS_Slowloris','FTP_Patator',
        'Heartbleed','Infiltration','PortScan','SSH_Patator','WebAttack_Brute_Force','WebAttack_Sql_Injection','WebAttack_XSS']
# SK-fold creation
skf = StratifiedKFold(n_splits=2,shuffle=True, random_state=42)
# ######################### FIRST ML MODEL STANDARD BAGGING ################################
# for train_index, val_index in skf.split(x_train,y_train):
#        # Use the below two lines if in dataframe else use the next two
#        # X_train_seg, X_val = X_train.iloc[train_index],X_train.iloc[val_index]
#        # y_train_seg, y_val = y_train.iloc[train_index], y_train.iloc[val_index]
#        X_train_seg, X_val = x_train[train_index], x_train[val_index]
#        y_train_seg, y_val = y_train[train_index], y_train[val_index]
#        # Create base classifier
#        model = BaggingClassifier(n_jobs=-1,verbose=2)
#        model.fit(X_train_seg, y_train_seg) # Fit data
#        y_val_pred = model.predict(X_val) # Predict labels for validation data
#        perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics
#
# # ######################### SECOND ML MODEL RANDOM FOREST WITH WEIGHT BALANCING #######################
# for train_index, val_index in skf.split(x_train,y_train):
#        # Use the below two lines if in dataframe else use the next two
#        # X_train_seg, X_val = X_train.iloc[train_index],X_train.iloc[val_index]
#        # y_train_seg, y_val = y_train.iloc[train_index], y_train.iloc[val_index]
#        X_train_seg, X_val = x_train[train_index], x_train[val_index]
#        y_train_seg, y_val = y_train[train_index], y_train[val_index]
#        # Create base classifier
#        model = RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=-1,verbose=2)
#        model.fit(X_train_seg, y_train_seg) # Fit data
#        y_val_pred = model.predict(X_val) # Predict labels for validation data
#        perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics


# ######################### THIRD ML MODEL RANDOM FOREST WITH BOOTSTRAP WEIGHT BALANCING #######################
# for train_index, val_index in skf.split(x_train,y_train):
#        # Use the below two lines if in dataframe else use the next two
#        # X_train_seg, X_val = X_train.iloc[train_index],X_train.iloc[val_index]
#        # y_train_seg, y_val = y_train.iloc[train_index], y_train.iloc[val_index]
#        X_train_seg, X_val = x_train[train_index], x_train[val_index]
#        y_train_seg, y_val = y_train[train_index], y_train[val_index]
#        # Create base classifier
#        model = RandomForestClassifier(n_estimators=100, class_weight='balanced_subsample', n_jobs=-1,verbose=2)
#        model.fit(X_train_seg, y_train_seg) # Fit data
#        y_val_pred = model.predict(X_val) # Predict labels for validation data
#        perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics
model = RandomForestClassifier(n_estimators=100, class_weight='balanced_subsample', n_jobs=-1,verbose=2)
model.fit(x_train, y_train) # Fit data
y_pred = model.predict(x_test)
perf_evaluation(y_test,y_pred,class_labels)
# ######################### FOURTH ML MODEL WEIGHTED RANDOM FOREST #######################
# for train_index, val_index in skf.split(x_train,y_train):
#        # Use the below two lines if in dataframe else use the next two
#        # X_train_seg, X_val = X_train.iloc[train_index],X_train.iloc[val_index]
#        # y_train_seg, y_val = y_train.iloc[train_index], y_train.iloc[val_index]
#        X_train_seg, X_val = x_train[train_index], x_train[val_index]
#        y_train_seg, y_val = y_train[train_index], y_train[val_index]
#        # Create base classifier
#        model = RandomForestClassifier(n_estimators=100, class_weight='balanced_subsample', n_jobs=-1)
#        model.fit(X_train_seg, y_train_seg) # Fit data
#        y_val_pred = model.predict(X_val) # Predict labels for validation data
#        perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics
