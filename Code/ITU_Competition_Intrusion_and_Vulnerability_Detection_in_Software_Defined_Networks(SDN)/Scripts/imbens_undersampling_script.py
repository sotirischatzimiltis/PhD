################### IMPORT NECESSARY PACKAGES ############################################
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
import seaborn as sns
import matplotlib.pyplot as plt
import imbens
from imbens.metrics import geometric_mean_score
from imbens.metrics import classification_report_imbalanced
from sklearn import preprocessing
from sklearn import tree
from sklearn.model_selection import StratifiedKFold
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
# Packages for under-sampling ensembles
from imbens.ensemble import BalanceCascadeClassifier
from imbens.ensemble import BalancedRandomForestClassifier
from imbens.ensemble import EasyEnsembleClassifier
from imbens.ensemble import RUSBoostClassifier
from imbens.ensemble import UnderBaggingClassifier
################### METHOD THAT PRINTS ALL EVALUATION METRICS NEEDED #####################
def perf_evaluation(y_true,y_pred,class_labels):
    # # Compute the confusion matrix
    matrix = confusion_matrix(y_true,y_pred,normalize='true')
    conf_matrix = confusion_matrix(y_true,y_pred)
    print("####################################################################################")
    print("Confusion Matrix:")
    # Plot the confusion matrix as a heatmap
    plt.figure(figsize=(15, 15))
    sns.heatmap(matrix, annot=True,fmt='.4f',cmap="viridis", square=True,
                xticklabels=class_labels, yticklabels=class_labels)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix')
    plt.show()
    # Accuracy and Classification Report
    accuracy = accuracy_score(y_true, y_pred) * 100
    print("Total Accuracy: ",accuracy)
    report = classification_report(y_true,y_pred)
    print("Classification Report")
    print(report)
    # Classification Report based on the imbens library
    report = classification_report_imbalanced(y_true, y_pred)
    print("Classification Report based on imbens")
    print(report)
    # Calculate TP, TN, FP, FN for each class
    class_tp = {}
    class_tn = {}
    class_fp = {}
    class_fn = {}
    for i, label in enumerate(class_labels):
        tp = conf_matrix[i, i]
        tn = conf_matrix.sum() - conf_matrix[i, :].sum() - conf_matrix[:, i].sum() + tp
        fp = conf_matrix[:, i].sum() - tp
        fn = conf_matrix[i, :].sum() - tp
        class_tp[label] = tp
        class_tn[label] = tn
        class_fp[label] = fp
        class_fn[label] = fn
    # Print true positives, true negatives, false positives, and false negatives for each class
    for label in class_labels:
        print(f"Class {label}:")
        print("True Positives (TP):", class_tp[label])
        print("True Negatives (TN):", class_tn[label])
        print("False Positives (FP):", class_fp[label])
        print("False Negatives (FN):", class_fn[label])
        print()

########################### MAIN CODE ####################################################
#################### READ DATASETS #######################################################
train_file_path = 'Datasets/cleaned_dataset_train.csv'
test_file_path = 'Datasets/cleaned_dataset_test.csv'
train_df = pd.read_csv(train_file_path,skipinitialspace=True)
test_df = pd.read_csv(test_file_path,skipinitialspace=True)
#Split data into X(input features) and Y (labels)
X_train = train_df.drop(train_df.columns[-1], axis=1)
y_train = train_df[train_df.columns[-1]]
X_test = test_df.drop(test_df.columns[-1], axis=1)
y_test = test_df[test_df.columns[-1]]
# Scale data
scaler = preprocessing.StandardScaler()
scaler.fit(X_train)
x_train = scaler.transform(X_train)
x_test = scaler.transform(X_test)
# Save feature and class names
features = X_train.columns.tolist()
class_labels = ['Benign','Bot','DDoS','DoS_Golden_Eye','DoS_Hulk','DoS_Slowhttptest','DoS_Slowloris','FTP_Patator',
        'Heartbleed','Infiltration','PortScan','SSH_Patator','WebAttack_Brute_Force','WebAttack_Sql_Injection','WebAttack_XSS']
# SK-fold creation
skf = StratifiedKFold(n_splits=2,shuffle=True, random_state=42)
############## UNDERSAMPLING BASED ENSEMBLES #############################################
######################### MODELS WITH Self Paced Ensemble Classifier #####################
########################### FIRST ML MODEL DECISION TREES ################################
for train_index, val_index in skf.split(x_train,y_train):
       # Use the below two lines if in dataframe else use the next two
       # X_train_seg, X_val = X_train.iloc[train_index],X_train.iloc[val_index]
       # y_train_seg, y_val = y_train.iloc[train_index], y_train.iloc[val_index]
       X_train_seg, X_val = x_train[train_index], x_train[val_index]
       y_train_seg, y_val = y_train[train_index], y_train[val_index]
       # Create base classifier
       DT_clf = tree.DecisionTreeClassifier()
       # Create ensemble classifier with 5 estimators and base classifier the DT_clf
       clf = imbens.ensemble.SelfPacedEnsembleClassifier(
           n_estimators=5,
           estimator=DT_clf,
       )
       clf.fit(X_train_seg, y_train_seg) # Fit data
       y_val_pred = clf.predict(X_val) # Predict labels for validation data
       perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics
########################### SECOND ML MODEL RANDOM FOREST ##########################################
for train_index, val_index in skf.split(x_train,y_train):
       # Use the below two lines if in dataframe else use the next two
       # X_train_seg, X_val = X_train.iloc[train_index],X_train.iloc[val_index]
       # y_train_seg, y_val = y_train.iloc[train_index], y_train.iloc[val_index]
       X_train_seg, X_val = x_train[train_index], x_train[val_index]
       y_train_seg, y_val = y_train[train_index], y_train[val_index]
       # Create base classifier
       RF_clf = RandomForestClassifier(n_estimators=100, max_depth=8,min_samples_split=2, random_state=0,verbose=2,n_jobs=-1)
       # Create ensemble classifier with 5 estimators and base classifier the DT_clf
       clf = imbens.ensemble.SelfPacedEnsembleClassifier(
           n_estimators=5,
           estimator=RF_clf,
       )
       clf.fit(X_train_seg, y_train_seg) # Fit data
       y_val_pred = clf.predict(X_val) # Predict labels for validation data
       perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics


######################### MODELS WITH Balanced Cascade Classifier #####################
########################### FIRST ML MODEL DECISION TREES ################################
for train_index, val_index in skf.split(x_train,y_train):
       # Use the below two lines if in dataframe else use the next two
       # X_train_seg, X_val = X_train.iloc[train_index],X_train.iloc[val_index]
       # y_train_seg, y_val = y_train.iloc[train_index], y_train.iloc[val_index]
       X_train_seg, X_val = x_train[train_index], x_train[val_index]
       y_train_seg, y_val = y_train[train_index], y_train[val_index]
       # Create base classifier
       DT_clf = tree.DecisionTreeClassifier()
       # Create ensemble classifier with 5 estimators and base classifier the DT_clf
       clf = BalanceCascadeClassifier(estimator=DT_clf,random_state=0,n_jobs=-1)
       clf.fit(X_train_seg, y_train_seg) # Fit data
       y_val_pred = clf.predict(X_val) # Predict labels for validation data
       perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics
# ########################### SECOND ML MODEL RANDOM FOREST###############################
for train_index, val_index in skf.split(x_train,y_train):
       # Use the below two lines if in dataframe else use the next two
       # X_train_seg, X_val = X_train.iloc[train_index],X_train.iloc[val_index]
       # y_train_seg, y_val = y_train.iloc[train_index], y_train.iloc[val_index]
       X_train_seg, X_val = x_train[train_index], x_train[val_index]
       y_train_seg, y_val = y_train[train_index], y_train[val_index]
       # Create base classifier
       RF_clf = RF_clf = RandomForestClassifier(n_estimators=20, max_depth=None,min_samples_split=2, random_state=0,verbose=2,n_jobs=-1)
       # Create ensemble classifier with 5 estimators and base classifier the DT_clf
       clf = BalanceCascadeClassifier(estimator=RF_clf,random_state=0,n_jobs=-1)
       clf.fit(X_train_seg, y_train_seg) # Fit data
       y_val_pred = clf.predict(X_val) # Predict labels for validation data
       perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics


######################### MODELS WITH Balanced Random Forest Classifier ##################
for train_index, val_index in skf.split(x_train,y_train):
       # Use the below two lines if in dataframe else use the next two
       # X_train_seg, X_val = X_train.iloc[train_index],X_train.iloc[val_index]
       # y_train_seg, y_val = y_train.iloc[train_index], y_train.iloc[val_index]
       X_train_seg, X_val = x_train[train_index], x_train[val_index]
       y_train_seg, y_val = y_train[train_index], y_train[val_index]
       # Create ensemble classifier with 5 estimators and base classifier the DT_clf
       clf = BalancedRandomForestClassifier(random_state=0,n_jobs=-1)
       clf.fit(X_train_seg, y_train_seg) # Fit data
       y_val_pred = clf.predict(X_val) # Predict labels for validation data
       perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics

######################### MODELS WITH Easy Ensemble Classifier ###########################
for train_index, val_index in skf.split(x_train,y_train):
       # Use the below two lines if in dataframe else use the next two
       # X_train_seg, X_val = X_train.iloc[train_index],X_train.iloc[val_index]
       # y_train_seg, y_val = y_train.iloc[train_index], y_train.iloc[val_index]
       X_train_seg, X_val = x_train[train_index], x_train[val_index]
       y_train_seg, y_val = y_train[train_index], y_train[val_index]
       # Create ensemble classifier with 5 estimators and base classifier the DT_clf
       clf = EasyEnsembleClassifier(random_state=0)
       clf.fit(X_train_seg, y_train_seg) # Fit data
       y_val_pred = clf.predict(X_val) # Predict labels for validation data
       perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics

######################### MODELS WITH RUS Boost Classifier ##################### BAD
for train_index, val_index in skf.split(x_train,y_train):
       # Use the below two lines if in dataframe else use the next two
       # X_train_seg, X_val = X_train.iloc[train_index],X_train.iloc[val_index]
       # y_train_seg, y_val = y_train.iloc[train_index], y_train.iloc[val_index]
       X_train_seg, X_val = x_train[train_index], x_train[val_index]
       y_train_seg, y_val = y_train[train_index], y_train[val_index]
       # Create ensemble classifier with 5 estimators and base classifier the DT_clf
       clf = RUSBoostClassifier(random_state=0)
       clf.fit(X_train_seg, y_train_seg) # Fit data
       y_val_pred = clf.predict(X_val) # Predict labels for validation data
       perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics

######################### MODELS WITH Under Bagging Classifier #####################
for train_index, val_index in skf.split(x_train,y_train):
       # Use the below two lines if in dataframe else use the next two
       # X_train_seg, X_val = X_train.iloc[train_index],X_train.iloc[val_index]
       # y_train_seg, y_val = y_train.iloc[train_index], y_train.iloc[val_index]
       X_train_seg, X_val = x_train[train_index], x_train[val_index]
       y_train_seg, y_val = y_train[train_index], y_train[val_index]
       # Create ensemble classifier with 5 estimators and base classifier the DT_clf
       clf = UnderBaggingClassifier(random_state=0)
       clf.fit(X_train_seg, y_train_seg) # Fit data
       y_val_pred = clf.predict(X_val) # Predict labels for validation data
       perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics