############################ Import Libraries ############################################
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import StratifiedKFold
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_validate
import time
import psutil
from imbens.metrics import classification_report_imbalanced
from sklearn import preprocessing
from tabulate import tabulate
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, f1_score
############################# Read Dataset################################################
train_file_path = '../Datasets/cleaned_dataset_train.csv'
test_file_path = '../Datasets/cleaned_dataset_test.csv'
train_df = pd.read_csv(train_file_path,skipinitialspace=True)
test_df = pd.read_csv(test_file_path,skipinitialspace=True)

#Split data into X(input features) and Y (labels)
X_train = train_df.drop(train_df.columns[-1], axis=1)
y_train = train_df[train_df.columns[-1]]
X_test = test_df.drop(test_df.columns[-1], axis=1)
y_test = test_df[test_df.columns[-1]]

# Scale data
scaler = preprocessing.StandardScaler()
scaler.fit(X_train)
x_train = scaler.transform(X_train)
x_test = scaler.transform(X_test)

features = X_train.columns.tolist()
class_labels = ['Benign','Bot','DDoS','DoS_Golden_Eye','DoS_Hulk','DoS_Slowhttptest','DoS_Slowloris','FTP_Patator',
        'Heartbleed','Infiltration','PortScan','SSH_Patator','WebAttack_Brute_Force','WebAttack_Sql_Injection','WebAttack_XSS']
########################### Performance evaluation method#################################
def perf_evaluation(y_true,y_pred,class_labels,plot=False):
    # # Compute the confusion matrix
    matrix = confusion_matrix(y_true,y_pred,normalize='true')
    conf_matrix = confusion_matrix(y_true,y_pred)
    print("####################################################################################")
    if plot:
        print("Confusion Matrix:")
        # Plot the confusion matrix as a heatmap
        plt.figure(figsize=(15, 15))
        sns.heatmap(matrix, annot=True,fmt='.4f',cmap="viridis", square=True,
                    xticklabels=class_labels, yticklabels=class_labels)
        plt.xlabel('Predicted Labels')
        plt.ylabel('True Labels')
        plt.title('Confusion Matrix')
        plt.show()
    # Accuracy and Classification Report
    accuracy = accuracy_score(y_true, y_pred) * 100
    print("Total Accuracy: ",accuracy)
    report = classification_report(y_true,y_pred)
    print("Classification Report")
    print(report)
    # Classification Report based on the imbens library
    report_imb = classification_report_imbalanced(y_true, y_pred)
    print("Classification Report based on imbens")
    print(report_imb)
    if plot:
        # Calculate TP, TN, FP, FN for each class
        class_tp = {}
        class_tn = {}
        class_fp = {}
        class_fn = {}
        for i, label in enumerate(class_labels):
            tp = conf_matrix[i, i]
            tn = conf_matrix.sum() - conf_matrix[i, :].sum() - conf_matrix[:, i].sum() + tp
            fp = conf_matrix[:, i].sum() - tp
            fn = conf_matrix[i, :].sum() - tp
            class_tp[label] = tp
            class_tn[label] = tn
            class_fp[label] = fp
            class_fn[label] = fn
        # Print true positives, true negatives, false positives, and false negatives for each class
        for label in class_labels:
            print(f"Class {label}:")
            print("True Positives (TP):", class_tp[label])
            print("True Negatives (TN):", class_tn[label])
            print("False Positives (FP):", class_fp[label])
            print("False Negatives (FN):", class_fn[label])
            print()
    return matrix, report_imb
############# Performance evaluation auxiliary method to combine the folds################
# array of confusion matrices, one for each fold
# list of reports, one for each fold
def perf_evaluation_auxiliary(matrices,reports,class_labels):
    stack_matrices = np.stack(matrices)
    average_matrix = np.mean(stack_matrices, axis=0)
    plt.figure(figsize=(15, 15))
    sns.heatmap(average_matrix, annot=True, fmt='.4f', cmap="viridis", square=True,
                xticklabels=class_labels, yticklabels=class_labels)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix')
    plt.show()

    # Initialize a dictionary to store the sum of metrics for each label
    sum_metrics = []
    headers = []
    # Count the number of dictionaries
    num_reports = len(reports)
    # Loop through each string report in the list
    for string_report in reports:
        # Since each report is a giant string first we need to bring it to the correct format
        lines = string_report.strip().split('\n') # Split string by lines
        headers = lines[0].split() # extracts the headers
        data_list = []
        for line in lines[2:-2]:
            values = line.split() # Split the line by spaces
            metrics = [float(value) for value in   values[1:]]  # Convert numeric values to float
            data_list.append(metrics)  # Add metrics to the data array
        data_array = np.array(data_list)
        sum_metrics.append(data_array)
    array_stack = np.stack(sum_metrics)
    average_report = np.mean(sum_metrics,axis=0)
    table = tabulate(average_report,headers=headers,tablefmt='pretty',floatfmt=".3f")
    print(table)
######################## Initialize Stratified 5-fold CV #################################
skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision_macro',
    'recall': 'recall_macro',
    'f1': 'f1_macro'
}
################################ BASELINE MODELS #########################################
###################### USING 5-FOLD CV WE ARE GETTING THE BEST PERFORMING MODEL ##########
################# AND THEN DO A FINAL EVALUATION ON THE TEST SET  ########################

############################### DECISION TREE ###########################################
############################### GRID SEARCH CV ##########################################
from sklearn import tree
clf = tree.DecisionTreeClassifier()
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
scoring_metric = make_scorer(f1_score, average='macro')
grid_search = GridSearchCV(clf, param_grid, cv=5,scoring=scoring_metric,n_jobs=-1)
grid_search.fit(X_train, y_train)
# Print the best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
dt_params = grid_search.best_params_
################################ AVERAGE PERFORMANCE USING 5-FOLD CV ####################
print("Decision Tree:")
print()
matrices = []
reports = []
for train_index, val_index in skf.split(x_train,y_train):
       X_train_seg, X_val = x_train[train_index], x_train[val_index]
       y_train_seg, y_val = y_train[train_index], y_train[val_index]
       # Create base classifier
       DT_clf = tree.DecisionTreeClassifier(**dt_params)
       DT_clf.fit(X_train_seg, y_train_seg) # Fit data
       y_val_pred = DT_clf.predict(X_val) # Predict labels for validation data
       m,r = perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics
       matrices.append(m)
       reports.append(r)
perf_evaluation_auxiliary(matrices,reports,class_labels)

DT_clf = tree.DecisionTreeClassifier(**dt_params)
results = cross_validate(DT_clf, X_train, y_train, cv=skf, scoring=scoring)
print("Cross-validation results:")
print("Accuracy:", results['test_accuracy'])
print("Mean Accuracy:", np.mean(results['test_accuracy']))
print("\nPrecision:", results['test_precision'])
print("Mean Precision:", np.mean(results['test_precision']))
print("\nRecall:", results['test_recall'])
print("Mean Recall:", np.mean(results['test_recall']))
print("\nF1-score:", results['test_f1'])
print("Mean F1-score:", np.mean(results['test_f1']))
##################  Test set evaluation of final model trained with full data ###########
start_time = time.time()
DT_clf = tree.DecisionTreeClassifier(**dt_params)
DT_clf.fit(X_train, y_train)
end_time = time.time()
dt_training_time = end_time - start_time
print("Decision Tree Training Time:", dt_training_time, "seconds")
y_pred = DT_clf.predict(X_test) # change to x_test_0
perf_evaluation(y_test,y_pred,class_labels,plot=True)

################################ K-NEAREST NEIGHBOURS ####################################
############################### GRID SEARCH CV ##########################################
# from sklearn.neighbors import KNeighborsClassifier
# clf = KNeighborsClassifier()
# param_grid = {
#     'n_neighbors': [3, 5],
#     'weights': ['uniform', 'distance'],
#     'p': [1, 2],    #1 for Manhattan distance, 2 for Euclidean distance
#
# }
# scoring_metric = 'f1' # evaluate based on f1-score since there is data imbalance
# grid_search = GridSearchCV(clf, param_grid, cv=5,scoring=scoring_metric,n_jobs=-1)
# grid_search.fit(X_train, y_train)
# # Print the best parameters and best score
# print("Best Parameters:", grid_search.best_params_)
# print("Best Score:", grid_search.best_score_)
# knn_params = grid_search.best_params_
################################# AVERAGE PERFORMANCE USING 5-FOLD CV ####################
# print("K-NN:")
# print()
# matrices = []
# reports = []
# for train_index, val_index in skf.split(x_train,y_train):
#        X_train_seg, X_val = x_train[train_index], x_train[val_index]
#        y_train_seg, y_val = y_train[train_index], y_train[val_index]
#        # Create base classifier
#        KNN_clf = KNeighborsClassifier(n_neighbors=3,n_jobs=-1,algorithm='kd_tree')
#        KNN_clf.fit(X_train_seg, y_train_seg) # Fit data
#        y_val_pred = KNN_clf.predict(X_val) # Predict labels for validation data
#        m,r = perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics
#        matrices.append(m)
#        reports.append(r)
# perf_evaluation_auxiliary(matrices,reports,class_labels)

# KNN_clf = KNeighborsClassifier(n_neighbors=3,n_jobs=-1,algorithm='kd_tree')
# results = cross_validate(KNN_clf, X_train, y_train, cv=skf, scoring=scoring)
# print("Cross-validation results:")
# print("Accuracy:", results['test_accuracy'])
# print("Mean Accuracy:", np.mean(results['test_accuracy']))
# print("\nPrecision:", results['test_precision'])
# print("Mean Precision:", np.mean(results['test_precision']))
# print("\nRecall:", results['test_recall'])
# print("Mean Recall:", np.mean(results['test_recall']))
# print("\nF1-score:", results['test_f1'])
# print("Mean F1-score:", np.mean(results['test_f1']))
###################  Test set evaluation of final model trained with full data ###########
# start_time = time.time()
# KNN_clf = (n_neighbors=3,n_jobs=-1,algorithm='kd_tree')
# KNN_clf.fit(X_train, y_train)
# end_time = time.time()
# KNN_training_time = end_time - start_time
# print("K-NN Training Time:", KNN_training_time, "seconds")
# y_pred = KNN_clf.predict(X_test) # change to x_test_0
# perf_evaluation(y_test,y_pred,class_labels,plot=True)

################################ RANDOM FOREST ###########################################
################################# AVERAGE PERFORMANCE USING 5-FOLD CV ####################
# print("Random Forest:")
# print()
# matrices = []
# reports = []
# from sklearn import tree
# for train_index, val_index in skf.split(x_train,y_train):
#        X_train_seg, X_val = x_train[train_index], x_train[val_index]
#        y_train_seg, y_val = y_train[train_index], y_train[val_index]
#        # Create base classifier
#        RF_clf = RandomForestClassifier(n_estimators=20, max_depth=None,min_samples_split=2, random_state=0,verbose=2,n_jobs=-1)
#        RF_clf.fit(X_train_seg, y_train_seg) # Fit data
#        y_val_pred = RF_clf.predict(X_val) # Predict labels for validation data
#        m,r = perf_evaluation(y_val,y_val_pred,class_labels) # Print performance metrics
#        matrices.append(m)
#        reports.append(r)
# perf_evaluation_auxiliary(matrices,reports,class_labels)

#from sklearn.ensemble import RandomForestClassifier
# RF_clf = RandomForestClassifier(n_estimators=100, max_depth=8,min_samples_split=2, random_state=0,verbose=2,n_jobs=-1)
# results = cross_validate(RF_clf, X_train, y_train, cv=skf, scoring=scoring)
# print("Cross-validation results:")
# print("Accuracy:", results['test_accuracy'])
# print("Mean Accuracy:", np.mean(results['test_accuracy']))
# print("\nPrecision:", results['test_precision'])
# print("Mean Precision:", np.mean(results['test_precision']))
# print("\nRecall:", results['test_recall'])
# print("Mean Recall:", np.mean(results['test_recall']))
# print("\nF1-score:", results['test_f1'])
# print("Mean F1-score:", np.mean(results['test_f1']))
###################  Test set evaluation of final model trained with full data ###########
# start_time = time.time()
# RF_clf = RandomForestClassifier(n_estimators=100, max_depth=8,min_samples_split=2, random_state=0,verbose=2,n_jobs=-1)
# RF_clf.fit(X_train, y_train)
# end_time = time.time()
# RF_training_time = end_time - start_time
# print("Random Forest Training Time:", RF_training_time, "seconds")
# y_pred = RF_clf.predict(X_test) # change to x_test_0
# perf_evaluation(y_test,y_pred,class_labels,plot=True)

