# -*- coding: utf-8 -*-
"""SplitLearning_part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TnNc1ueGo0lSvpszlfQd8PN7DgCCbbJu

#Scope


*   Understanding Split Learning
*   Setting up the Training Pipeline
*   Distributed Forward and Backward Pass
*   Scaling to multiple clients

#What is Split Learning?
Split Learning is a technique for distributed training and prediction without sharing raw data. The simplest idea here is to split a neural network architecture into two parts such that the first part can be executed by a trusted client and the output of the first part is fed to the second part which is typically executed by the sever.
This approach enables multi-client, multi-insitution collaboration for deep learning algorithms.

Set runtime as GPU
"""

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
import torch.optim as optim
from torch.autograd import Variable

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])

#CIFAR 10 is a dataset of natural images consisting of 50k training images and 10k test
#Every image is labelled with one of the following class
classes = ('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')

trainset = torchvision.datasets.CIFAR10(root='./data',train = True, download = True,transform = transform)
trainloader = torch.utils.data.DataLoader(trainset,batch_size=128,shuffle= True,num_workers = 2)

testset = torchvision.datasets.CIFAR10(root = './data', train = False,download = True,transform = transform)
testloader = torch.utils.data.DataLoader(testset,batch_size = 128,shuffle = True,num_workers = 2)

"""Pytorch provides nn.Module as a base class for all neural network modules. This allows easily creating a parameterized model.

torchvision package consists of popular datasets, model architectures and common image transformations for computer vision. Here we use torchvision's model API to use ResNet18 architecture.
"""

# Explain nn.Module and explain the forward and backward pass
class ResNet18Client(nn.Module):
    """ docstring for ResNet """
    #Explain initialize (listing the neural network architecture and other related parameters)
    def __init__(self,config):
        super(ResNet18Client,self).__init__()
        #Declare where we are going to split the NN, if cut layer close to output most computation will happen in the client side
        self.cut_layer = config["cut_layer"]

        #Load the resnet model
        self.model = models.resnet18(pretrained = False)
        self.model = nn.ModuleList(self.model.children())
        self.model = nn.Sequential(*self.model)

    def forward(self,x):
        for i,l in enumerate(self.model):
            if i> self.cut_layer:
                break
            x = l(x)
        return x

class ResNet18Server(nn.Module):
    """ docstring for ResNet """
    def __init__(self,config):
        super(ResNet18Server,self).__init__()
        self.logits = config["logits"]
        self.cut_layer = config["cut_layer"]

        self.model = models.resnet18(pretrained = False)
        num_ftrs = self.model.fc.in_features
        #
        self.model.fc = nn.Sequential(nn.Flatten(),nn.Linear(num_ftrs,self.logits))
        self.model = nn.ModuleList(self.model.children())
        self.model = nn.Sequential(*self.model)

    def forward(self,x):
        for i,l in enumerate(self.model):
            #continue until you are in the cut layer, skip earlier layers
            if i<=self.cut_layer:
                continue
            x = l(x)
        return nn.functional.softmax(x,dim=1)

"""#Initialize the Models"""

config = {'cut_layer':3,"logits":10} #logits = 10 cause we are using ciphar10 that has 10 classes
client_model = ResNet18Client(config).to(device)
server_model = ResNet18Server(config).to(device)

"""Set up the optimizer"""

criterion = nn.CrossEntropyLoss()
client_optimizer = optim.SGD(client_model.parameters(),lr=0.01,momentum=0.9)
server_optimizer = optim.SGD(server_model.parameters(),lr=0.01,momentum = 0.9)

"""Perform training"""

num_epochs = 50
for epoch in range(num_epochs):
    running_loss = 0.0
    for i,data in enumerate(trainloader,0):
        inputs, labels = data[0].to(device), data[1].to(device)

        client_optimizer.zero_grad()
        server_optimizer.zero_grad()

        #Client Part
        activation = client_model(inputs)
        server_inputs = activation.detach().clone() # get the outputs of the client and input them to the server

        #Simulation of sever part is happening in this portion
        #Server part
        server_inputs = Variable(server_inputs,requires_grad = True)
        outputs = server_model(server_inputs)
        loss = criterion(outputs,labels)
        loss.backward()

        #Server optimization
        server_optimizer.step()

        #Simulation of Client Happening in this portion
        #Client optimization
        activation.backward(server_inputs.grad)
        client_optimizer.step()

        running_loss += loss.item()

        if i % 200 == 199:
            print("[{},{}] loss: {}".format(epoch+1,i+1,running_loss/200))

# save the model
!mkdir = 'saved_models'

client_model_path = './saved_models/trained_client_model.pt'
server_model_path = './saved_models/trained_server_model.pt'
torch.save(client_model.state_dict(),client_model_path)
torch.save(server_model.state_dict(),server_model_path)

# Evaluation
total, correct = 0, 0
with torch.no_grad():
    for data in testloader:
        inputs, labels = data[0].to(device), data[1].to(device)
        outputs = server_model(client_model(inputs))
        _, predicted = torch.max(outputs.data,1)
        total += labels.size(0)
        correct += (predicted==labels).sum().item()
    print(correct/total)

"""# Scaling to multiple clients
We will implement a basic round robin protocol for multiple clients to perform distributed training.
Here each client takes part in training and then send its weight to the subsequent client who continues with the  training.
Last 50mins of https://www.youtube.com/watch?v=VPL6ELWdJbg
"""

total_client_num = 10
#Initialize multiple clients
client_model_list = [ResNet18Client(config).to(device) for client_num in range(total_client_num)]
#Initialize multiple optimizers
client_optimizer_list = [optim.SGD(client_model_list[client_num].parameters(),lr=0.01,momentum =0.9) for client_num in range(total_client_num)]
server = ResNet18Server(config).to(device)
server_optimizer = optim.SGD(server.parameters(),lr=0.01,momentum = 0.9)

"""#Training pipeline for multiple clients"""

num_epochs = 50 // total_client_num
for epoch in range(num_epochs):
    # Iterate over multiple clients
    for client_num in range(total_client_num):
        print("Current active client is {}".format(client_num))
        client = client_model_list[client_num]
        clinet_optimizer = client_optimizer_list[client_num]

        #Logic to load the weights from the previous client
        if client_num ==0:
            if epoch != 0:
                prev_client = total_client_num - 1
                prev_client_weights = client_model_list[prev_client].state_dict()
                client.load_state_dict(prev_client_weights)
                print("Loaded client {}'s weight successfully".format(prev_client))
        else:
            prev_client = client_num -1
            prev_client_weights = client_model_list[prev_client].state_dict()
            client.load_state_dict(prev_client_weights)
            print("Loaded client {}'s weight successfully".format(prev_client))

        client.train()
        running_loss = 0.0
        total_samples = 0
        # All clients have their own data source and we are using a single trainloader here for illustration fo te simulated setup
        for i, data in enumerate(trainloader,0):
            inputs, labels = data[0].to(device), data[1].to(device)

            client_optimizer.zero_grad()
            server_optimizer.zero_grad()

            # Client part
            activations = client(inputs)
            server_inputs = activations.detach().clone()

            # Server part
            server_inputs  = Variable(server_inputs,requires_grad = True)
            outputs = server(server_inputs)
            loss = criterion(outputs,labels)
            loss.backward()
            server_optimizer.step()

            running_loss += loss.item()
            total_samples += labels.shape[0]

            # Client part
            activations.backward(server_inputs.grad)
            client_optimizer.step()

            if i % 50 ==1:
                print( "Client: {}, Epoch: {}, Iteration: {}, Loss: {:.4f}".format(client_num,epoch,i,running_loss/(i+1)))

"""# Future Directions
1. How to make the round robin protocol asynchronous?
2. Different data distribution across clients?
3. Different topologies of client models, multiple servers, etc.
"""